{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be63d04d",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "An s3 bucket has lots of bloomberg files. Some files as .gz format. \n",
    "\n",
    "A bloomberg file is in a certain format. It has a listing of fields and then pipe-delimited data corresponding to the listing of fields. The first 3 columns in the pipe-delimited data is meta data.  From the 4th pipe-delimited column of row data, the columns match with the listing of fields.\n",
    "\n",
    "Below are screenshots from a sample bbg file:\n",
    "<img src=\"./images/1.jpg\" width=\"200\" height=\"400\"> \n",
    "![2](./images/2.jpg) \n",
    "![3](./images/3.jpg)\n",
    "\n",
    "Some files have the following after start-of-data that need to be dealt with:\n",
    "![4](./images/4.jpg)\n",
    "\n",
    "\n",
    "Among all the fields of each file, how do we fetch data for only 4 attributes - ID_ISIN, SECURITY_TYP , SECURITY_TYP2, CFI_CODE and output it into 1 file?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc40e98",
   "metadata": {},
   "source": [
    "# High-level solution approach\n",
    "\n",
    "1. Connect to AWS. For some reason adding it as part of the code would not work with my account. So before I ran the code below, I did the following in cmd: octo login aws and followed the steps. I set the aws token duration for 1 hour. It wouldnt allow for more than that. I choose 'default' profile in the last step.\n",
    "1. Get a list of all files within the s3 key (folder)\n",
    "1. For each file, check if all the following attributes are present - ID_ISIN, SECURITY_TYP , SECURITY_TYP2. CFI_CODE is optional. So the check did not comprise it also. \n",
    "1. Since the above step took a long time and for fear than further processing will be stalled due to an expired token, the files that met the check above were written into a file (file of all the applicable s3 key files).\n",
    "1. Add the content of the file in above step to a list\n",
    "1. For each item in list ... (an item in list is a file in s3 bucket)\n",
    "    1.1 create the field list from the bbg file \n",
    "    1.1 create a dictionary from the field list for the attributes of interest: ID_ISIN, SECURITY_TYP , SECURITY_TYP2. CFI_CODE. The key of the dictionary item is the attribute name. The value of the dictionary item is the index of the attribute in the field list .\n",
    "    1.1 create a panda dataframe with data of matching index columns from the BBG file. The columns of the dataframe are the attribute names (i.e. dictionary keys)\n",
    "    1.1 Append the panda dataframe for each BBG file to a list of dataframes\n",
    "1. After going through each item (s3 key file) in the list, create a dataframe that concatenates all the dataframes in the list along axis=0.\n",
    "1. Write this final dataframe into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e59f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3 is to interact with s3\n",
    "import boto3\n",
    "\n",
    "# not sure sys, os, csv is needed\n",
    "# import sys\n",
    "# import os\n",
    "# import csv\n",
    "\n",
    "# content of file will be read as string\n",
    "import io\n",
    "\n",
    "\n",
    "# content of file will be read into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# some s3 keys are gzip files. this is needed to read the content of these gzip files\n",
    "import gzip\n",
    "\n",
    "# to measure how long a file processing takes\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b26618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n",
      "['NT_BB/', 'NT_BB/InputFile.csv', 'NT_BB/QA.corpPfdAsiaV2.out.20220520.CorpConv Non-Euro Descriptive V2', 'NT_BB/QA.equity_asia1.out.20220527.Equity Descriptive', 'NT_BB/QA.equity_bdvd_asia1.out.20220520.Equity Dividend', 'NT_BB/QA.mtge_abs_namr.out.20220520.ABS CMBS CMO Loan Descriptive', 'NT_BB/QA.mtge_cmbs_namr.out.20220520.ABS CMBS CMO Loan Descriptive', 'NT_BB/QA.mtge_cmo_namr.out.20220520.ABS CMBS CMO Loan Descriptive', 'NT_BB/corpPfdAsiaV2.out.20220520', 'NT_BB/corpPfdEuroV2.out.20220520']\n",
      "Length of bucket_list 264\n"
     ]
    }
   ],
   "source": [
    "# get all the files in s3 bucket that are within folder NT_BB\n",
    "s3_client = boto3.client('s3')\n",
    "s3_bucket_name='conductor-data-input'\n",
    "s3=boto3.resource('s3')\n",
    "my_bucket=s3.Bucket(s3_bucket_name)\n",
    "bucket_list = []\n",
    "for file in my_bucket.objects.filter(Prefix = 'Hello/'): # Prefix changed\n",
    "    file_name=file.key\n",
    "    bucket_list.append(file.key)\n",
    "length_bucket_list=print(len(bucket_list))\n",
    "print(bucket_list[:10])\n",
    "print(\"Length of bucket_list\",len(bucket_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fa7e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NT_BB/QA.corpPfdAsiaV2.out.20220520.CorpConv Non-Euro Descriptive V2\n",
      "NT_BB/QA.equity_asia1.out.20220527.Equity Descriptive\n",
      "NT_BB/QA.mtge_abs_namr.out.20220520.ABS CMBS CMO Loan Descriptive\n",
      "NT_BB/QA.mtge_cmbs_namr.out.20220520.ABS CMBS CMO Loan Descriptive\n",
      "NT_BB/QA.mtge_cmo_namr.out.20220520.ABS CMBS CMO Loan Descriptive\n",
      "NT_BB/corpPfdAsiaV2.out.20220520\n",
      "NT_BB/corpPfdEuroV2.out.20220520\n",
      "NT_BB/corpPfdLamrV2.out.20220520\n",
      "NT_BB/corpPfdNamrV2.out.20220520\n",
      "NT_BB/corp_loan_fatca_us.out.20220527\n",
      "NT_BB/corp_pfd_asia.out.20220520\n",
      "NT_BB/corp_pfd_convert_asia.out.20220520\n",
      "NT_BB/corp_pfd_convert_asia.out.20220529\n",
      "NT_BB/corp_pfd_convert_euro.out.20220520\n",
      "NT_BB/corp_pfd_convert_euro.out.20220527.filepart\n",
      "NT_BB/corp_pfd_convert_lamr.out.20220522\n",
      "NT_BB/corp_pfd_convert_lamr.out.20220527\n",
      "NT_BB/corp_pfd_convert_namr.out.20220520\n",
      "NT_BB/corp_pfd_euro.out.20220520\n",
      "NT_BB/corp_pfd_euro.out.20220527\n",
      "NT_BB/corp_pfd_lamr.out.20220520\n",
      "NT_BB/corp_pfd_lamr.out.20220527\n",
      "NT_BB/corp_pfd_namr.out.20220520\n",
      "NT_BB/equityOptionsAsia1.out.20220527\n",
      "NT_BB/equityOptionsAsia2.out.20220527\n",
      "NT_BB/equityOptionsEuro1.out.20220527\n",
      "NT_BB/equityOptionsEvening1Lamr1.out.20220527\n",
      "NT_BB/equity_asia1.out.20220520\n",
      "NT_BB/equity_asia1.out.20220527\n",
      "NT_BB/equity_asia2.out.20220520\n",
      "NT_BB/equity_asia2.out.20220527\n",
      "NT_BB/equity_lamr.out.20220520\n",
      "NT_BB/equity_lamr.out.20220527\n",
      "NT_BB/equity_namr.out.20220520\n",
      "NT_BB/fixedIncomeBoAsiaV2.out.20220520\n",
      "NT_BB/fixedIncomeBoEuroV2.out.20220520\n",
      "NT_BB/fixedIncomeBoLamrV2.out.20220520\n",
      "NT_BB/fixedIncomeBoNamrV2.out.20220520\n",
      "NT_BB/govt_agency_regl_asia.out.20220520\n",
      "NT_BB/govt_agency_regl_euro.out.20220520\n",
      "NT_BB/govt_agency_regl_euro.out.20220527\n",
      "NT_BB/govt_agency_regl_lamr.out.20220520\n",
      "NT_BB/govt_agency_regl_lamr.out.20220527\n",
      "NT_BB/govt_agency_regl_namr.out.20220520\n",
      "NT_BB/govt_national_asia.out.20220520\n",
      "NT_BB/govt_national_asia.out.20220527\n",
      "NT_BB/govt_national_euro.out.20220520\n",
      "NT_BB/govt_national_euro.out.20220527\n",
      "NT_BB/govt_national_lamr.out.20220520\n",
      "NT_BB/govt_national_lamr.out.20220527\n",
      "NT_BB/govt_national_namr.out.20220520\n",
      "NT_BB/mtge_abs_namr.out.20220520\n",
      "NT_BB/mtge_abs_namr.out.20220520.dont.use\n",
      "NT_BB/mtge_canadian_namr.out.20220520\n",
      "NT_BB/mtge_cmbs_namr.out.20220520\n",
      "NT_BB/mtge_cmo_ext_namr.out.20220520\n",
      "NT_BB/mtge_cmo_namr.out.20220520\n",
      "NT_BB/mtge_fnma_ext_namr.out.20220520\n",
      "NT_BB/mtge_fnma_namr.out.20220520\n",
      "NT_BB/mtge_generic_ext_namr.out.20220520\n",
      "NT_BB/mtge_generic_namr.out.20220520\n",
      "NT_BB/mtge_gnma_ext_namr.out.20220520\n",
      "NT_BB/mtge_sba_ext_namr.out.20220520\n",
      "NT_BB/mtge_sba_namr.out.20220520\n",
      "NT_BB/mtge_tba_ext_namr.out.20220520\n",
      "NT_BB/mtge_tba_namr.out.20220520\n",
      "NT_BB/mtge_wholeloan_ext_namr.out.20220520\n",
      "NT_BB/mtge_wholeloan_namr.out.20220520\n",
      "NT_BB/pfd_bo_asia.out.20220520\n",
      "NT_BB/pfd_bo_euro.out.20220520\n",
      "NT_BB/pfd_bo_lamr.out.20220520\n",
      "NT_BB/pfd_bo_namr.out.20220520\n"
     ]
    }
   ],
   "source": [
    "# finds files that have following 3 attributes - ID_ISIN, SECURITY_TYP, SECURITY_TYP2\n",
    "\n",
    "filteredBucketList=[] \n",
    "for b in bucket_list:\n",
    "    obj=s3.Object(s3_bucket_name,b)\n",
    "    if 'gz' in b: # if the file is compressed as .gz\n",
    "        with gzip.GzipFile(fileobj=obj.get()[\"Body\"]) as gzipfile:\n",
    "            content = gzipfile.read(5*1024**2).decode(\"utf-8\")\n",
    "            if all(x in content for x in ['ID_ISIN', 'SECURITY_TYP','SECURITY_TYP2']):\n",
    "                filteredBucketList.append(b)\n",
    "                print(\"Compressed File: \", b)\n",
    "    else:\n",
    "        data=obj.get()['Body'].read().decode('utf-8')\n",
    "        if all(x in data for x in ['ID_ISIN', 'SECURITY_TYP', 'SECURITY_TYP2']):\n",
    "            filteredBucketList.append(b)\n",
    "            print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c150cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#write  files to a txt file that have fields 'ID_ISIN', 'SECURITY_TYP','SECURITY_TYP2'\n",
    "with open(r'C:\\Users\\mtm\\Desktop\\DeleteLater\\ForA\\s3KeyListWISINSecTypSecTyp2.txt', 'w') as outfile:\n",
    "    for item in filteredBucketList:\n",
    "        outfile.write(\"%s\\n\" % item)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "598b5013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "['NT_BB/QA.corpPfdAsiaV2.out.20220520.CorpConv Non-Euro Descriptive V2\\n', 'NT_BB/QA.equity_asia1.out.20220527.Equity Descriptive\\n', 'NT_BB/QA.mtge_abs_namr.out.20220520.ABS CMBS CMO Loan Descriptive\\n', 'NT_BB/QA.mtge_cmbs_namr.out.20220520.ABS CMBS CMO Loan Descriptive\\n', 'NT_BB/QA.mtge_cmo_namr.out.20220520.ABS CMBS CMO Loan Descriptive\\n', 'NT_BB/corpPfdAsiaV2.out.20220520\\n', 'NT_BB/corpPfdEuroV2.out.20220520\\n', 'NT_BB/corpPfdLamrV2.out.20220520\\n', 'NT_BB/corpPfdNamrV2.out.20220520\\n', 'NT_BB/corp_loan_fatca_us.out.20220527\\n']\n",
      "72\n",
      "['NT_BB/QA.corpPfdAsiaV2.out.20220520.CorpConv Non-Euro Descriptive V2', 'NT_BB/QA.equity_asia1.out.20220527.Equity Descriptive', 'NT_BB/QA.mtge_abs_namr.out.20220520.ABS CMBS CMO Loan Descriptive', 'NT_BB/QA.mtge_cmbs_namr.out.20220520.ABS CMBS CMO Loan Descriptive', 'NT_BB/QA.mtge_cmo_namr.out.20220520.ABS CMBS CMO Loan Descriptive', 'NT_BB/corpPfdAsiaV2.out.20220520', 'NT_BB/corpPfdEuroV2.out.20220520', 'NT_BB/corpPfdLamrV2.out.20220520', 'NT_BB/corpPfdNamrV2.out.20220520', 'NT_BB/corp_loan_fatca_us.out.20220527']\n"
     ]
    }
   ],
   "source": [
    "# write the s3 keys that have ID_ISIN, SEC_TYP, SEC_TYP2 in content into list - filesWithISINSecTypSecTyp2\n",
    "filesWithISINSecTypSecTyp2=[]\n",
    "infile=open(r'C:\\Users\\mtm\\Desktop\\DeleteLater\\ForA\\s3KeyListWISINSecTypSecTyp2.txt', 'r')\n",
    "filesWithISINSecTypSecTyp2=infile.readlines()\n",
    "print(len(filesWithISINSecTypSecTyp2))\n",
    "print(filesWithISINSecTypSecTyp2[:10])\n",
    "# remove \\n character in each list item\n",
    "newfilesWithISINSecTypSecTyp2 = [x[:-1] for x in filesWithISINSecTypSecTyp2]\n",
    "print(len(newfilesWithISINSecTypSecTyp2))\n",
    "print(newfilesWithISINSecTypSecTyp2[:10])\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7fbce5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newfilesWithISINSecTypSecTyp2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# For the filtered list find files that have CFI_CODE and add such files to list - filteredBucketList2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m filteredBucketList2\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnewfilesWithISINSecTypSecTyp2\u001b[49m:\n\u001b[0;32m      4\u001b[0m     obj\u001b[38;5;241m=\u001b[39ms3\u001b[38;5;241m.\u001b[39mObject(s3_bucket_name,b)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgz\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m b:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'newfilesWithISINSecTypSecTyp2' is not defined"
     ]
    }
   ],
   "source": [
    "# For the filtered list find files that have CFI_CODE and add such files to list - filteredBucketList2\n",
    "filteredBucketList2=[]\n",
    "for b in newfilesWithISINSecTypSecTyp2:\n",
    "    obj=s3.Object(s3_bucket_name,b)\n",
    "    if 'gz' in b:\n",
    "        with gzip.GzipFile(fileobj=obj.get()[\"Body\"]) as gzipfile:\n",
    "            content = gzipfile.read(5*1024**2).decode(\"utf-8\")\n",
    "            if all(x in content for x in ['CFI_CODE']):\n",
    "                filteredBucketList2.append(b)\n",
    "                print(\"Compressed File: \", b)\n",
    "    else:\n",
    "        data=obj.get()['Body'].read().decode('utf-8')\n",
    "        if all(x in data for x in ['CFI_CODE']):\n",
    "            filteredBucketList2.append(b)\n",
    "            print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write such files that have CFI_CODE to a txt file\n",
    "with open(r'C:\\Users\\mtm\\Desktop\\DeleteLater\\ForA\\s3KeyListWISINSecTypSecTyp2CFICode.txt', 'w') as outfile:\n",
    "    for item in filteredBucketList2:\n",
    "        outfile.write(\"%s\\n\" % item)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16efd6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write files with 'ID_ISIN', 'SECURITY_TYP','SECURITY_TYP2', 'CFI_CODE' into a list\n",
    "file=r'C:\\Users\\mtm\\Desktop\\DeleteLater\\ForA\\s3KeyListWISINSecTypSecTyp2CFICode.txt'\n",
    "infile=open(file,'r')\n",
    "listOfFilesWith4Attributes=infile.readlines()\n",
    "print(listOfFilesWith4Attributes[:10])\n",
    "# remove \\n character in each list item\n",
    "listOfFilesWith4Attributes=[item.strip('\\n') for item in listOfFilesWith4Attributes]\n",
    "print(listOfFilesWith4Attributes[:10])\n",
    "print (len(listOfFilesWith4Attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of files comparing 2 input lists - filesWithISINSecTypSecTyp2 ; listOfFilesWith4Attributes. \n",
    "# The new list has entries not in both the input lists.\n",
    "\n",
    "def listDifference(l1, l2):\n",
    "    ld = [] # ld - list difference \n",
    "    ld = [x for x in l1 + l2 if x in l1 and x not in l2]\n",
    "    print (\"length of difference in lists: \", len(ld))\n",
    "    return ld\n",
    "\n",
    "differenceInList = listDifference(newfilesWithISINSecTypSecTyp2, listOfFilesWith4Attributes)\n",
    "print (differenceInList[:10])\n",
    "print (len(differenceInList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3873b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the list of the difference between the 2 lists into a file\n",
    "def writeToFile(l,filename):\n",
    "    str = r'C:\\Users\\mtm\\Desktop\\DeleteLater\\ForA\\\\'+filename\n",
    "    with open(str, 'w') as outfile:\n",
    "        for item in l:\n",
    "            outfile.write(\"%s\\n\" % item)\n",
    "    print(\"Done\")\n",
    "    \n",
    "writeToFile(differenceInList,'filesWithNoCFICode.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa578caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdataFields(data):\n",
    "    # get the data fields from the file\n",
    "    \n",
    "    datafields=''\n",
    "    fieldL=[]\n",
    "    \n",
    "    # for 1 string, we pull string between 'start-of-fields' and 'end of fields'\n",
    "    startString='START-OF-FIELDS'\n",
    "    endString='END-OF-FIELDS'\n",
    "    \n",
    "    startPoint=data.index(startString)\n",
    "    endPoint=data.index(endString)\n",
    "    \n",
    "    datafields=data[startPoint + len(startString) + 1:endPoint-1]\n",
    "    \n",
    "    fieldL=datafields.split('\\n')\n",
    "    fieldL = [item for item in fieldL if not item.startswith('#')]\n",
    "    \n",
    "    while(\"\" in fieldL):\n",
    "            fieldL.remove(\"\")\n",
    "\n",
    "    \n",
    "    return fieldL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRowData(data):\n",
    "# for 2nd string, we pull string between 'start-of-data' and 'end of data'\n",
    "    \n",
    "    rD=''\n",
    "\n",
    "    startString='START-OF-DATA'\n",
    "    endString='END-OF-DATA'\n",
    "    \n",
    "    startPoint=data.index(startString)\n",
    "    \n",
    "    if endString not in data:\n",
    "        endPoint=0\n",
    "    else:\n",
    "        endPoint=data.index(endString)\n",
    "      \n",
    "    \n",
    "    rD=data[startPoint + len(startString) + 1:endPoint-1] # rd - rowData\n",
    "    \n",
    "    if '# ' in rD[:5]:\n",
    "        print('# present in 1st row Data')\n",
    "    rD=rD.split('\\n',1)[1]\n",
    "    \n",
    "    return rD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232258d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictionaryofFieldNameAndIndexForFields(fL):\n",
    "    \"\"\"get the indices of the data fields\"\"\"\n",
    "    dicForSelectFields=dict() # dictionary to store the field name and index\n",
    "    for i in fL:\n",
    "        if (i in ['ID_ISIN', 'SECURITY_TYP','SECURITY_TYP2', 'CFI_CODE']):\n",
    "            dicForSelectFields[i]=fL.index(i)\n",
    "    # Add 3 to each index for select fields in dictionary\n",
    "    for k in dicForSelectFields:\n",
    "        dicForSelectFields[k]=dicForSelectFields[k]+3\n",
    "    return dicForSelectFields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the file list and extract the file into a df. All 10 dfs into a list. Concatenate it as 1 df\n",
    "# write the df of 10 files into a file. print done for each file of 10 dfs\n",
    "\n",
    "outputDF=[] # a list of dataframes\n",
    "c = 1 # counter to count how many files of 10 dfs get printed\n",
    "path = 'C:\\\\Users\\\\mtm\\\\Desktop\\\\DeleteLater\\\\ForA\\\\ExtractedFilesAll4Fields\\\\'\n",
    "\n",
    "fileListToLoop = newfilesWithISINSecTypSecTyp2\n",
    "\n",
    "# remove bad file - name changed\n",
    "fileListToLoop.remove('Hello/tes.123')\n",
    "\n",
    "for i  in fileListToLoop:\n",
    "    # time the loop iteration start\n",
    "    tic = time.time()\n",
    "    # get data for file\n",
    "    obj = s3.Object(s3_bucket_name,i)\n",
    "    data=obj.get()['Body'].read().decode('utf-8')\n",
    "    # get data fields\n",
    "    fieldList=getdataFields(data)\n",
    "    # create dictionary of feildList value - 'ID_ISIN', 'SECURITY_TYP','SECURITY_TYP2', 'CFI_CODE' - and thier index value \n",
    "    fieldDictionary=createDictionaryofFieldNameAndIndexForFields(fieldList)\n",
    "    # get row data\n",
    "    rowData=getRowData(data)\n",
    "    # add row data to itemDF dataframe\n",
    "    rowDataAsString=io.StringIO(rowData)\n",
    "    rowDataDf=pd.read_csv(rowDataAsString, header=None, delimiter=\"|\", low_memory=False)\n",
    "    # create df of select colum data and column header\n",
    "    tdf=rowDataDf[fieldDictionary.values()] # tdf - target df with headers and columns needed\n",
    "    tdf.set_axis(fieldDictionary.keys(),axis=1,inplace=True)\n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "    # append itemDF to outputDF . Print 10 times at a time to 1 file\n",
    "    outputDF.append(tdf)\n",
    "    \n",
    "    # time the loop iteration end\n",
    "    toc = time.time()\n",
    "    \n",
    "    duration = toc - tic\n",
    "    # print data fram of file added to list\n",
    "    print('Printed ' , c, ' file. Filename: ', i, 'Iteration duration: {0:2f}'.format(duration))\n",
    "    c+=1\n",
    "   \n",
    "\n",
    "out=pd.concat(outputDF, ignore_index = True)\n",
    "abspath=path+str(c)+'.csv'\n",
    "out.to_csv(abspath)\n",
    "\n",
    "del out\n",
    "outputDF=[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
